{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62c80835",
   "metadata": {},
   "source": [
    "**Visual Agentic AI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86012526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain==0.3.24 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.3.24)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\ASUS\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: langchain-openai==0.3.14 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.3.14)\n",
      "Requirement already satisfied: langgraph==0.3.33 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.3.33)\n",
      "Requirement already satisfied: langchain-tavily==0.1.6 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.1.6)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.55 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain==0.3.24) (0.3.61)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain==0.3.24) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain==0.3.24) (0.3.31)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain==0.3.24) (2.11.3)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain==0.3.24) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain==0.3.24) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain==0.3.24) (6.0.2)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.68.2 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-openai==0.3.14) (1.82.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-openai==0.3.14) (0.9.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.10 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langgraph==0.3.33) (2.0.26)\n",
      "Requirement already satisfied: langgraph-prebuilt<0.2,>=0.1.8 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langgraph==0.3.33) (0.1.8)\n",
      "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langgraph==0.3.33) (0.1.70)\n",
      "Requirement already satisfied: xxhash<4.0.0,>=3.5.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langgraph==0.3.33) (3.5.0)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.11.14 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-tavily==0.1.6) (3.11.16)\n",
      "Requirement already satisfied: mypy<2.0.0,>=1.15.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-tavily==0.1.6) (1.15.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily==0.1.6) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily==0.1.6) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily==0.1.6) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily==0.1.6) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily==0.1.6) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily==0.1.6) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily==0.1.6) (1.20.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-core<1.0.0,>=0.3.55->langchain==0.3.24) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-core<1.0.0,>=0.3.55->langchain==0.3.24) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-core<1.0.0,>=0.3.55->langchain==0.3.24) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-core<1.0.0,>=0.3.55->langchain==0.3.24) (4.13.2)\n",
      "Requirement already satisfied: ormsgpack<2.0.0,>=1.8.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph==0.3.33) (1.9.1)\n",
      "Requirement already satisfied: httpx>=0.25.2 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.3.33) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.3.33) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.24) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.24) (0.23.0)\n",
      "Requirement already satisfied: mypy_extensions>=1.0.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mypy<2.0.0,>=1.15.0->langchain-tavily==0.1.6) (1.0.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai==0.3.14) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai==0.3.14) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai==0.3.14) (0.10.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai==0.3.14) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai==0.3.14) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.24) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.24) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.24) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2->langchain==0.3.24) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2->langchain==0.3.24) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2->langchain==0.3.24) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2->langchain==0.3.24) (2025.1.31)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from SQLAlchemy<3,>=1.4->langchain==0.3.24) (3.2.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tiktoken<1,>=0.7->langchain-openai==0.3.14) (2024.11.6)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.3.33) (1.0.8)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.3.33) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.55->langchain==0.3.24) (3.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm>4->openai<2.0.0,>=1.68.2->langchain-openai==0.3.14) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U langchain==0.3.24 langchain-openai==0.3.14 langgraph==0.3.33 langchain-tavily==0.1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a9ea22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"LANGCHAIN_API_KEY\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Visual-Agentic-AI\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"TavilyAPIKey\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"OPENAI_API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "643850b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily  import TavilySearch\n",
    "\n",
    "web_search =  TavilySearch(max_results=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92107b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overview of Vietnam Weather. Average temperature: 27 °C (81 °F), varies by region and season. Average humidity: 84%, varies by region and season. Average rainfall: 1,800 mm (71 inches), varies by region and season. Average sunshine duration: 2,000 hours per year, varies by region and season Average wind speed: not available. Rainy Season: May to October in most of the country, influenced by\n"
     ]
    }
   ],
   "source": [
    "web_search_results = web_search.invoke(\"What is the temperature of Vietnam?\")\n",
    "\n",
    "print(web_search_results[\"results\"][0][\"content\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42803686",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "research_agent= create_react_agent(\n",
    "    model = \"gpt-4o-mini\",\n",
    "    tools = [web_search],\n",
    "    prompt=(\n",
    "        \"You are a research agent.\\n\\n\"\n",
    "        \"INSTRUCTIONS:\\n\"\n",
    "        \"- Assist ONLY with research-related tasks, DO NOT do any math\\n\"\n",
    "        \"- After you're done with your tasks, respond to the supervisor directly\\n\"\n",
    "        \"- Respond ONLY with the results of your work, do NOT include ANY other text.\"\n",
    "    ),\n",
    "    name=\"research_agent\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6673a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import convert_to_messages\n",
    "\n",
    "def pretty_print_message(message, indent = False):\n",
    "    pretty_message = message.pretty_repr(html=True)\n",
    "    if not indent:\n",
    "        print(pretty_message)\n",
    "        return\n",
    "    intended = \"\\n\".join(\"\\t\" + c for c in pretty_message.split(\"\\n\"))\n",
    "    print(intended)\n",
    "\n",
    "def pretty_print_messages(update, cast_message = False):\n",
    "    is_subgraph = False\n",
    "    if isinstance(update, tuple):\n",
    "        namespace, update = update\n",
    "        if len(namespace) == 0:\n",
    "            return\n",
    "        graph_id = namespace[-1].split(\":\")[0]\n",
    "        print(f\"Update from subgraph {graph_id}:\\n\")\n",
    "        print(\"\\n\")\n",
    "        is_subgraph = True\n",
    "\n",
    "    for node_name, node_update in update.items():\n",
    "        update_label = f\"Update from node {node_name}:\\n\"\n",
    "        if is_subgraph:\n",
    "            update_label = \"\\t\" + update_label\n",
    "        print(update_label)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        messages = convert_to_messages(node_update[\"messages\"])\n",
    "        for message in messages:\n",
    "            pretty_print_message(message, indent = is_subgraph)\n",
    "        print(\"\\n\") \n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3eea4287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update from node agent:\n",
      "\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: research_agent\n",
      "Tool Calls:\n",
      "  tavily_search (call_nOm8hoh0qLahwcV3VolOv1Q5)\n",
      " Call ID: call_nOm8hoh0qLahwcV3VolOv1Q5\n",
      "  Args:\n",
      "    query: current Mayor of New York City\n",
      "    search_depth: basic\n",
      "\n",
      "\n",
      "Update from node tools:\n",
      "\n",
      "\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search\n",
      "\n",
      "{\"query\": \"current Mayor of New York City\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"title\": \"Office of the Mayor | Mayor's Bio | City of New York - NYC.gov\", \"url\": \"https://www.nyc.gov/office-of-the-mayor/bio.page\", \"content\": \"Mayor Eric Adams has served the people of New York City as an NYPD officer, State Senator, Brooklyn Borough President, and now as the 110th Mayor of the City of New York. He gave voice to a diverse coalition of working families in all five boroughs and is leading the fight to bring back New York City's economy, reduce inequality, improve public safety, and build a stronger, healthier city\", \"score\": 0.8479807, \"raw_content\": null}, {\"title\": \"Eric Adams (New York) - Ballotpedia\", \"url\": \"https://ballotpedia.org/Eric_Adams_(New_York)\", \"content\": \"Eric Adams (Democratic Party) is the Mayor of New York. He assumed office on January 1, 2022. His current term ends on January 1, 2026. Adams (independent) is running for re-election for Mayor of New York. He declared candidacy for the general election scheduled on November 4, 2025. Adams was born in Brooklyn in 1960. He earned an associate arts degree from New York City Technical College, a\", \"score\": 0.8296166, \"raw_content\": null}, {\"title\": \"Eric Adams - Wikipedia\", \"url\": \"https://en.wikipedia.org/wiki/Eric_Adams\", \"content\": \"Eric Leroy Adams (born September 1, 1960) is an American politician and former police officer who has served as the 110th mayor of New York City since 2022. Adams was an officer in the New York City Transit Police and then the New York City Police Department (NYPD) for more than 20 years, retiring at the rank of captain.\", \"score\": 0.77487355, \"raw_content\": null}, {\"title\": \"Eric Adams | Political Party, Facts, Mayor, & Indictment | Britannica\", \"url\": \"https://www.britannica.com/biography/Eric-Adams\", \"content\": \"Eric Adams (b. 1960) is the 110th mayor of New York City, a position he has held since 2022. He previously worked in the New York City Police Department for more than 20 years. His tenure as mayor has been filled with controversies, and in September 2024 he was indicted on federal charges of bribery, fraud, and soliciting foreign campaign donations.\", \"score\": 0.70944256, \"raw_content\": null}, {\"title\": \"Who's who in Eric Adams' administration - City & State New York\", \"url\": \"https://www.cityandstateny.com/politics/2025/01/whos-who-eric-adams-administration/360056/\", \"content\": \"Who's who in Eric Adams' administration From deputy mayors to fixers to legal counsel, here are the insiders the mayor has appointed to run the city.\", \"score\": 0.21143347, \"raw_content\": null}], \"response_time\": 1.97}\n",
      "\n",
      "\n",
      "Update from node agent:\n",
      "\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: research_agent\n",
      "\n",
      "The current Mayor of New York City is Eric Adams. He has been in office since January 1, 2022.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk in research_agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Who is the Mayor of NYC?\"}]} \n",
    "):\n",
    "    pretty_print_messages(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a504efc",
   "metadata": {},
   "source": [
    "**Math Agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dfc18fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a: float, b: float):\n",
    "    \"\"\"Add two numbers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "def multiply(a: float, b: float):\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "def divide(a: float, b: float):\n",
    "    \"\"\"Divide two numbers.\"\"\"\n",
    "    return a / b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "56b473e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "math_agent = create_react_agent(\n",
    "    model = \"gpt-4o-mini\",\n",
    "    tools = [add, multiply, divide],\n",
    "    prompt = (\n",
    "        \"You are a math agent.\\n\\n\"\n",
    "        \"INSTRUCTIONS:\\n\"\n",
    "        \"- Assist ONLY with math-related tasks\\n\"\n",
    "        \"- After you're done with your tasks, respond to the supervisor directly\\n\"\n",
    "        \"- Respond ONLY with the results of your work, do NOT include ANY other text.\"\n",
    "    ),\n",
    "    name=\"math_agent\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "39d0041a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update from node agent:\n",
      "\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: math_agent\n",
      "Tool Calls:\n",
      "  add (call_pEB4WudyG53jWxhsxfxrxrM7)\n",
      " Call ID: call_pEB4WudyG53jWxhsxfxrxrM7\n",
      "  Args:\n",
      "    a: 4\n",
      "    b: 5\n",
      "\n",
      "\n",
      "Update from node tools:\n",
      "\n",
      "\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: add\n",
      "\n",
      "9.0\n",
      "\n",
      "\n",
      "Update from node agent:\n",
      "\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: math_agent\n",
      "\n",
      "9.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk in math_agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is 4 + 5?\"}]}\n",
    "):\n",
    "    pretty_print_messages(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dd6f13a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph_supervisor import create_supervisor\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "supervisor = create_supervisor(\n",
    "    model = init_chat_model(\"gpt-4o-mini\"),\n",
    "    agents = [math_agent, research_agent],\n",
    "    prompt=(\n",
    "        \"You are a supervisor managing two agents:\\n\"\n",
    "        \"- a research agent. Assign research-related tasks to this agent\\n\"\n",
    "        \"- a math agent. Assign math-related tasks to this agent\\n\"\n",
    "        \"Assign work to one agent at a time, do not call agents in parallel.\\n\"\n",
    "        \"Do not do any work yourself.\"\n",
    "    ),\n",
    "    add_handoff_back_messages=True,\n",
    "    output_mode=\"full_history\",\n",
    ").compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2f4244a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "display(Image(supervisor.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6b4424e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'supervisor': {'messages': [HumanMessage(content='What is the GPD of Vietnam in 2011, predict it in 2025 in billion USD?', additional_kwargs={}, response_metadata={}, id='4d9f648c-5a4f-4885-8305-ca7b66f1ced7'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_znKGWKCqhCAbxHpUp4N1ywiZ', 'function': {'arguments': '{}', 'name': 'transfer_to_math_agent'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 141, 'total_tokens': 153, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_54eb4bd693', 'id': 'chatcmpl-BbMpONoNmF1c69fXQ6yAt07cnfFPk', 'finish_reason': 'tool_calls', 'logprobs': None}, name='supervisor', id='run--af6a658c-4140-4f85-927e-7953d1028fd0-0', tool_calls=[{'name': 'transfer_to_math_agent', 'args': {}, 'id': 'call_znKGWKCqhCAbxHpUp4N1ywiZ', 'type': 'tool_call'}], usage_metadata={'input_tokens': 141, 'output_tokens': 12, 'total_tokens': 153, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='Successfully transferred to math_agent', name='transfer_to_math_agent', id='a9f9a0aa-9859-4806-b92a-e3e1a9dc79bc', tool_call_id='call_znKGWKCqhCAbxHpUp4N1ywiZ')]}}\n",
      "{'math_agent': {'messages': [HumanMessage(content='What is the GPD of Vietnam in 2011, predict it in 2025 in billion USD?', additional_kwargs={}, response_metadata={}, id='4d9f648c-5a4f-4885-8305-ca7b66f1ced7'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_znKGWKCqhCAbxHpUp4N1ywiZ', 'function': {'arguments': '{}', 'name': 'transfer_to_math_agent'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 141, 'total_tokens': 153, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_54eb4bd693', 'id': 'chatcmpl-BbMpONoNmF1c69fXQ6yAt07cnfFPk', 'finish_reason': 'tool_calls', 'logprobs': None}, name='supervisor', id='run--af6a658c-4140-4f85-927e-7953d1028fd0-0', tool_calls=[{'name': 'transfer_to_math_agent', 'args': {}, 'id': 'call_znKGWKCqhCAbxHpUp4N1ywiZ', 'type': 'tool_call'}], usage_metadata={'input_tokens': 141, 'output_tokens': 12, 'total_tokens': 153, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='Successfully transferred to math_agent', name='transfer_to_math_agent', id='a9f9a0aa-9859-4806-b92a-e3e1a9dc79bc', tool_call_id='call_znKGWKCqhCAbxHpUp4N1ywiZ'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_7dwTvQahcZkprMm8RguxY77t', 'function': {'arguments': '{\"a\": 137.22, \"b\": 1.2}', 'name': 'multiply'}, 'type': 'function'}, {'id': 'call_4pR2YATJBBtC8EEgWOZ5Gl16', 'function': {'arguments': '{\"a\": 229.5, \"b\": 1.2}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 58, 'prompt_tokens': 186, 'total_tokens': 244, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_62a23a81ef', 'id': 'chatcmpl-BbMpP7JLudsY8jZb7WSPj1oIHXlpx', 'finish_reason': 'tool_calls', 'logprobs': None}, name='math_agent', id='run--dc0dbbac-5b89-47d7-ba24-ea47b3a27213-0', tool_calls=[{'name': 'multiply', 'args': {'a': 137.22, 'b': 1.2}, 'id': 'call_7dwTvQahcZkprMm8RguxY77t', 'type': 'tool_call'}, {'name': 'multiply', 'args': {'a': 229.5, 'b': 1.2}, 'id': 'call_4pR2YATJBBtC8EEgWOZ5Gl16', 'type': 'tool_call'}], usage_metadata={'input_tokens': 186, 'output_tokens': 58, 'total_tokens': 244, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='164.664', name='multiply', id='d57fd5cb-adc8-465d-81fc-fe335dd6b838', tool_call_id='call_7dwTvQahcZkprMm8RguxY77t'), ToolMessage(content='275.4', name='multiply', id='2bcc3d94-28b8-4343-8015-535f37fe5ce1', tool_call_id='call_4pR2YATJBBtC8EEgWOZ5Gl16'), AIMessage(content='164.664 billion USD (2011 GDP) and predicted 275.4 billion USD (2025 GDP).', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 267, 'total_tokens': 291, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_62a23a81ef', 'id': 'chatcmpl-BbMpRGHk7pkYpRxQ1uMJjo1vsuxFe', 'finish_reason': 'stop', 'logprobs': None}, name='math_agent', id='run--69cea11d-463b-4f98-85ee-3097be046ba1-0', usage_metadata={'input_tokens': 267, 'output_tokens': 24, 'total_tokens': 291, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), AIMessage(content='Transferring back to supervisor', additional_kwargs={}, response_metadata={'__is_handoff_back': True}, name='math_agent', id='7ffaa120-283e-48ae-9be4-b768adab802c', tool_calls=[{'name': 'transfer_back_to_supervisor', 'args': {}, 'id': 'f750d5a4-770d-474b-b83e-f8c8ef660653', 'type': 'tool_call'}]), ToolMessage(content='Successfully transferred back to supervisor', name='transfer_back_to_supervisor', id='612c7061-dfe1-4138-9c5d-2069cbc14a17', tool_call_id='f750d5a4-770d-474b-b83e-f8c8ef660653')]}}\n",
      "{'supervisor': {'messages': [HumanMessage(content='What is the GPD of Vietnam in 2011, predict it in 2025 in billion USD?', additional_kwargs={}, response_metadata={}, id='4d9f648c-5a4f-4885-8305-ca7b66f1ced7'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_znKGWKCqhCAbxHpUp4N1ywiZ', 'function': {'arguments': '{}', 'name': 'transfer_to_math_agent'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 141, 'total_tokens': 153, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_54eb4bd693', 'id': 'chatcmpl-BbMpONoNmF1c69fXQ6yAt07cnfFPk', 'finish_reason': 'tool_calls', 'logprobs': None}, name='supervisor', id='run--af6a658c-4140-4f85-927e-7953d1028fd0-0', tool_calls=[{'name': 'transfer_to_math_agent', 'args': {}, 'id': 'call_znKGWKCqhCAbxHpUp4N1ywiZ', 'type': 'tool_call'}], usage_metadata={'input_tokens': 141, 'output_tokens': 12, 'total_tokens': 153, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='Successfully transferred to math_agent', name='transfer_to_math_agent', id='a9f9a0aa-9859-4806-b92a-e3e1a9dc79bc', tool_call_id='call_znKGWKCqhCAbxHpUp4N1ywiZ'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_7dwTvQahcZkprMm8RguxY77t', 'function': {'arguments': '{\"a\": 137.22, \"b\": 1.2}', 'name': 'multiply'}, 'type': 'function'}, {'id': 'call_4pR2YATJBBtC8EEgWOZ5Gl16', 'function': {'arguments': '{\"a\": 229.5, \"b\": 1.2}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 58, 'prompt_tokens': 186, 'total_tokens': 244, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_62a23a81ef', 'id': 'chatcmpl-BbMpP7JLudsY8jZb7WSPj1oIHXlpx', 'finish_reason': 'tool_calls', 'logprobs': None}, name='math_agent', id='run--dc0dbbac-5b89-47d7-ba24-ea47b3a27213-0', tool_calls=[{'name': 'multiply', 'args': {'a': 137.22, 'b': 1.2}, 'id': 'call_7dwTvQahcZkprMm8RguxY77t', 'type': 'tool_call'}, {'name': 'multiply', 'args': {'a': 229.5, 'b': 1.2}, 'id': 'call_4pR2YATJBBtC8EEgWOZ5Gl16', 'type': 'tool_call'}], usage_metadata={'input_tokens': 186, 'output_tokens': 58, 'total_tokens': 244, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='164.664', name='multiply', id='d57fd5cb-adc8-465d-81fc-fe335dd6b838', tool_call_id='call_7dwTvQahcZkprMm8RguxY77t'), ToolMessage(content='275.4', name='multiply', id='2bcc3d94-28b8-4343-8015-535f37fe5ce1', tool_call_id='call_4pR2YATJBBtC8EEgWOZ5Gl16'), AIMessage(content='164.664 billion USD (2011 GDP) and predicted 275.4 billion USD (2025 GDP).', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 267, 'total_tokens': 291, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_62a23a81ef', 'id': 'chatcmpl-BbMpRGHk7pkYpRxQ1uMJjo1vsuxFe', 'finish_reason': 'stop', 'logprobs': None}, name='math_agent', id='run--69cea11d-463b-4f98-85ee-3097be046ba1-0', usage_metadata={'input_tokens': 267, 'output_tokens': 24, 'total_tokens': 291, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), AIMessage(content='Transferring back to supervisor', additional_kwargs={}, response_metadata={'__is_handoff_back': True}, name='math_agent', id='7ffaa120-283e-48ae-9be4-b768adab802c', tool_calls=[{'name': 'transfer_back_to_supervisor', 'args': {}, 'id': 'f750d5a4-770d-474b-b83e-f8c8ef660653', 'type': 'tool_call'}]), ToolMessage(content='Successfully transferred back to supervisor', name='transfer_back_to_supervisor', id='612c7061-dfe1-4138-9c5d-2069cbc14a17', tool_call_id='f750d5a4-770d-474b-b83e-f8c8ef660653'), AIMessage(content='The GDP of Vietnam in 2011 was 164.664 billion USD, and it is predicted to be approximately 275.4 billion USD in 2025.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 35, 'prompt_tokens': 327, 'total_tokens': 362, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_54eb4bd693', 'id': 'chatcmpl-BbMpmurxdz3Ged2etpnyZf69tbhIj', 'finish_reason': 'stop', 'logprobs': None}, name='supervisor', id='run--838bcd1a-ad73-49ce-b00e-0a7074817cdc-0', usage_metadata={'input_tokens': 327, 'output_tokens': 35, 'total_tokens': 362, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n"
     ]
    }
   ],
   "source": [
    "for chunk in supervisor.stream(\n",
    "    {\n",
    "        \"messages\" : [\n",
    "            {\n",
    "                \"role\" : \"user\",\n",
    "                \"content\" : \"What is the GPD of Vietnam in 2011, predict it in 2025 in billion USD?\",\n",
    "\n",
    "            }\n",
    "        ]\n",
    "    },  \n",
    "    \n",
    "):\n",
    "    pretty_print_messages(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e186b7",
   "metadata": {},
   "source": [
    "***Multi Agent Supervisor***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "71cf9de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: arxiv in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.2.0)\n",
      "Collecting wikipedia\n",
      "  Using cached wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: duckduckgo-search in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (8.0.2)\n",
      "Requirement already satisfied: feedparser~=6.0.10 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from arxiv) (6.0.11)\n",
      "Requirement already satisfied: requests~=2.32.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from arxiv) (2.32.3)\n",
      "Collecting beautifulsoup4 (from wikipedia)\n",
      "  Downloading beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: click>=8.1.8 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from duckduckgo-search) (8.1.8)\n",
      "Requirement already satisfied: primp>=0.15.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from duckduckgo-search) (0.15.0)\n",
      "Requirement already satisfied: lxml>=5.3.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from duckduckgo-search) (5.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from click>=8.1.8->duckduckgo-search) (0.4.6)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests~=2.32.0->arxiv) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests~=2.32.0->arxiv) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests~=2.32.0->arxiv) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests~=2.32.0->arxiv) (2025.1.31)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->wikipedia)\n",
      "  Downloading soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from beautifulsoup4->wikipedia) (4.13.2)\n",
      "Downloading beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "Downloading soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (setup.py): started\n",
      "  Building wheel for wikipedia (setup.py): finished with status 'done'\n",
      "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11703 sha256=b9db7e8e2cdd8c2fb8198a5e8235468cfcf3bf857beaa6eb1b989924e42bf8bd\n",
      "  Stored in directory: c:\\users\\asus\\appdata\\local\\pip\\cache\\wheels\\8f\\ab\\cb\\45ccc40522d3a1c41e1d2ad53b8f33a62f394011ec38cd71c6\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: soupsieve, beautifulsoup4, wikipedia\n",
      "Successfully installed beautifulsoup4-4.13.4 soupsieve-2.7 wikipedia-1.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\ASUS\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install arxiv wikipedia duckduckgo-search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb51574",
   "metadata": {},
   "source": [
    "**Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b714669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import ArxivQueryRun, WikipediaQueryRun, DuckDuckGoSearchRun\n",
    "from langchain_community.utilities import ArxivAPIWrapper, WikipediaAPIWrapper, DuckDuckGoSearchAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4f60360e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Paris is the capital of what country? Paris, city and capital of France, situated in the north-central part of the country. People were living on the site of the present-day city, located along the Seine River some 233 miles (375 km) upstream from the river\\'s mouth on the English Channel (La Manche), by about 7600 bce. Learn about Paris, the capital and largest city of France, with its history, culture, landmarks, and transportation. Find out its location, population, currency, and weather in 2025. What is the French Capital of the World? The answer, unequivocally, is Paris. While the term \"French capital of the world\" isn\\'t an official designation, it\\'s a moniker that Paris has earned through its immense influence on global culture, fashion, art, cuisine, and politics. Learn why Paris is the capital of France and how it became the political and cultural hub of the country. Discover the best places to visit, the weather, the food, and the nicknames of the \"City of Light\" and the \"City of Love\". The capital of France is Paris, and there are no other official capitals. However, there are several other major cities in France that are important, such as: Marseille. Lyon. Toulouse. But Paris is the only official capital city.'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DuckDuckGo_wrapper = DuckDuckGoSearchAPIWrapper()\n",
    "DuckDuckGo = DuckDuckGoSearchRun(\n",
    "    api_wrapper = DuckDuckGo_wrapper,\n",
    "    description=\"Use this tool to search for information on the internet\"\n",
    ")\n",
    "DuckDuckGo.invoke(\"What is the capital of France?\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ac8451cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Published: 2025-03-03\\nTitle: Rotary Outliers and Rotary Offset Features in Large Language Models\\nAuthors: André Jonasson\\nSummary: Transformer-based Large Language Models (LLMs) rely on positional encodings\\nto provide sequence position information to their attention mechanism. Rotary\\nPositional Encodings (RoPE), which encode relative position by rotating queries\\nand keys, have become widely used in modern LLMs. We study the features and\\npatterns that emerge in queries and keys when using rotary embeddings. Our\\nanalysis reveals consistent patterns within the same model across layers and\\nattention heads and across different models and architectures. We present and\\napply analysis techniques and show how the queries and keys use RoPE to\\nconstruct various attention patterns, including attention sinks. We find and\\nanalyze outliers across models in queries and keys and find that they are\\nlikely to be found in rotary features with partial cycles. We derive bounds\\nthat tell us what rotary frequencies are likely to be selected as outlier\\nfeatures and at what minimum angle the query-key rotary pairs in these features\\ntend to be above and verify the bounds empirically with models of significant\\narchitectural differences.\\n\\nPublished: 2024-06-14\\nTitle: 3D-RPE: Enhancing Long-Context Modeling Through 3D Rotary Position Encoding\\nAuthors: Xindian Ma, Wenyuan Liu, Peng Zhang, Nan Xu\\nSummary: Inspired by the Bloch Sphere representation, we propose a novel rotary\\nposition encoding on a three-dimensional sphere, named 3D Rotary Position\\nEncoding (3D-RPE). 3D-RPE is an advanced version of the widely used 2D Rotary\\nPosition Encoding (RoPE), with two major advantages for modeling long contexts:\\ncontrollable long-term decay and improved position resolution. For controllable\\nlong-term decay, 3D-RPE allows for the regulation of long-term decay within the\\nchunk size, ensuring the modeling of relative positional information between\\ntokens at a distant relative position. For enhanced position resolution, 3D-RPE\\ncan mitigate the degradation of position resolution caused by position\\ninterpolation on RoPE. We have conducted experiments on long-context Natural\\nLanguage Understanding (NLU) and long-sequence Language Modeling (LM) tasks.\\nFrom the experimental results, 3D-RPE achieved performance improvements over\\nRoPE, especially in long-context NLU tasks.\\n\\nPublished: 2025-05-21\\nTitle: VRoPE: Rotary Position Embedding for Video Large Language Models\\nAuthors: Zikang Liu, Longteng Guo, Yepeng Tang, Tongtian Yue, Junxian Cai, Kai Ma, Qingbin Liu, Xi Chen, Jing Liu\\nSummary: Rotary Position Embedding (RoPE) has shown strong performance in text-based\\nLarge Language Models (LLMs), but extending it to video remains a challenge due\\nto the intricate spatiotemporal structure of video frames. Existing\\nadaptations, such as RoPE-3D, attempt to encode spatial and temporal dimensions\\nseparately but suffer from two major limitations: positional bias in attention\\ndistribution and disruptions in video-text transitions. To overcome these\\nissues, we propose Video Rotary Position Embedding (VRoPE), a novel positional\\nencoding method tailored for Video-LLMs. Specifically, we introduce a more\\nbalanced encoding strategy that mitigates attention biases, ensuring a more\\nuniform distribution of spatial focus. Additionally, our approach restructures\\npositional indices to ensure a smooth transition between video and text tokens.\\nExtensive experiments on different models demonstrate that VRoPE consistently\\noutperforms previous RoPE variants, achieving significant improvements in video\\nunderstanding, temporal reasoning, and retrieval tasks. Code will be available\\nat https://github.com/johncaged/VRoPE.'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_wrapper = ArxivAPIWrapper(\n",
    "    top_k_result = 4,\n",
    "    doc_content_chars_min = 0,\n",
    "    doc_content_chars_max = 2000\n",
    ")\n",
    "arxiv = ArxivQueryRun(\n",
    "    arxiv_wrapper = arxiv_wrapper,\n",
    "    description=\"Use this tool to search for papers on arxiv.org\"\n",
    ")\n",
    "arxiv.invoke(\"Rotary Positional Encoding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "729c5f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Page: Deep learning\\nSummary: Deep learning is a subset of machine learning that focuses on utilizing multilayered neural networks to perform tasks such as classification, regression, and representation learning. The field takes inspiration from biological neuroscience and is centered around stacking artificial neurons into layers and \"training\" them to process data. The adjective \"deep\" refers to the use of multiple layers (ranging from three to several hundred or thousands) in the network. Methods used can be either supervised, semi-supervised or unsupervised.\\nSome common deep learning network architectures include fully connected networks, deep belief networks, recurrent neural networks, convolutional neural networks, generative adversarial networks, transformers, and neural radiance fields. These architectures have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.\\nEarly forms of neural networks were inspired by information processing and distributed communication nodes in biological systems, particularly the human brain. However, current neural networks do not intend to model the brain function of organisms, and are generally seen as low-quality models for that purpose.\\n\\n\\n\\nPage: Transformer (deep learning architecture)\\nSummary: The transformer is a deep learning architecture that was developed by researchers at Google and is based on the multi-head attention mechanism, which was proposed in the 2017 paper \"Attention Is All You Need\". Text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism, allowing the signal for key tokens to be amplified and less important tokens to be diminished.\\nTransformers have the advantage of having no recurrent units, therefore requiring less training time than earlier recurrent neural architectures (RNNs) such as long short-term memory (LSTM). Later variations have been widely adopted for training large language models (LLM) on large (language) datasets.\\n\\nTransformers were first developed as an improvement over previous architectures for machine translation, but have found many applications since. They are used in large-scale natural language processing, computer vision (vision transformers), reinforcement learning, audio, multimodal learning, robotics, and even playing chess. It has also led to the development of pre-trained systems, such as generative pre-trained transformers (GPTs) and BERT (bidirectional encoder representations from transformers).\\n\\nPage: Deep Learning Super Sampling\\nSummary: Deep Learning Super Sampling (DLSS) is a suite of real-time deep learning image enhancement and upscaling technologies developed by Nvidia that are available in a number of video games. The goal of these technologies is to allow the majority of the graphics pipeline to run at a lower resolution for increased performance, and then infer a higher resolution image from this that approximates the same level of detail as if the image had been rendered at this higher resolution. This allows for higher graphical settings and/or frame rates for a given output resolution, depending on user preference.\\nAll generations of DLSS are available on all RTX-branded cards from Nvidia in supported titles. However, the Frame Generation feature is only supported on 40 series GPUs or newer and Multi Frame Generation is only available on 50 series GPUs.\\n\\n'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia_wrapper = WikipediaAPIWrapper()\n",
    "wikipedia = WikipediaQueryRun(\n",
    "    api_wrapper = wikipedia_wrapper,\n",
    "    description=\"Use this tool to search for information on wikipedia\"\n",
    ")\n",
    "wikipedia.invoke(\"Deep Learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e3779c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "baddad6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import convert_to_messages\n",
    "\n",
    "def pretty_print_message(message, indent=False):\n",
    "    pretty_message = message.pretty_repr(html=True)\n",
    "    if not indent:\n",
    "        print(pretty_message)\n",
    "        return\n",
    "\n",
    "    indented = \"\\n\".join(\"\\t\" + c for c in pretty_message.split(\"\\n\"))\n",
    "    print(indented)\n",
    "\n",
    "def pretty_print_messages(update, last_message=False):\n",
    "    is_subgraph = False\n",
    "    if isinstance(update, tuple):\n",
    "        ns, update = update\n",
    "        # skip parent graph updates in the printouts\n",
    "        if len(ns) == 0:\n",
    "            return\n",
    "\n",
    "        graph_id = ns[-1].split(\":\")[0]\n",
    "        print(f\"Update from subgraph {graph_id}:\")\n",
    "        print(\"\\n\")\n",
    "        is_subgraph = True\n",
    "\n",
    "    for node_name, node_update in update.items():\n",
    "        update_label = f\"Update from node {node_name}:\"\n",
    "        if is_subgraph:\n",
    "            update_label = \"\\t\" + update_label\n",
    "\n",
    "        print(update_label)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        messages = convert_to_messages(node_update[\"messages\"])\n",
    "        if last_message:\n",
    "            messages = messages[-1:]\n",
    "\n",
    "        for m in messages:\n",
    "            pretty_print_message(m, indent=is_subgraph)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e2375c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "research_agent = create_react_agent(\n",
    "    model = \"gpt-4o-mini\",\n",
    "    tools = [arxiv, DuckDuckGo, wikipedia],\n",
    "    prompt = (\"you are a research agent.\\n\\n\"\n",
    "    \"INSTRUCTIONS:\\n\"\n",
    "    \"1. Read the question carefully.\\n\"\n",
    "    \"2. Use the tools to find the answer.\\n\"\n",
    "    \"3. Return the answer.\\n\\n\"),\n",
    "    name = \"research_agent\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2e35e99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update from node agent:\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: research_agent\n",
      "Tool Calls:\n",
      "  wikipedia (call_AHY1cqhPeSYR7NP8L1ZhjiUH)\n",
      " Call ID: call_AHY1cqhPeSYR7NP8L1ZhjiUH\n",
      "  Args:\n",
      "    query: Reinforcement Learning\n",
      "\n",
      "\n",
      "Update from node tools:\n",
      "\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: wikipedia\n",
      "\n",
      "Page: Reinforcement learning\n",
      "Summary: Reinforcement learning (RL) is an interdisciplinary area of machine learning and optimal control concerned with how an intelligent agent should take actions in a dynamic environment in order to maximize a reward signal. Reinforcement learning is one of the three basic machine learning paradigms, alongside supervised learning and unsupervised learning.\n",
      "Reinforcement learning differs from supervised learning in not needing labelled input-output pairs to be presented, and in not needing sub-optimal actions to be explicitly corrected. Instead, the focus is on finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge) with the goal of maximizing the cumulative reward (the feedback of which might be incomplete or delayed). The search for this balance is known as the exploration–exploitation dilemma.\n",
      "\n",
      "The environment is typically stated in the form of a Markov decision process (MDP), as many reinforcement learning algorithms use dynamic programming techniques. The main difference between classical dynamic programming methods and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the Markov decision process, and they target large MDPs where exact methods become infeasible. \n",
      "\n",
      "\n",
      "\n",
      "Page: Deep reinforcement learning\n",
      "Summary: Deep reinforcement learning (DRL) is a subfield of machine learning that combines principles of reinforcement learning (RL) and deep learning. It involves training agents to make decisions by interacting with an environment to maximize cumulative rewards, while using deep neural networks to represent policies, value functions, or environment models. This integration enables DRL systems to process high-dimensional inputs, such as images or continuous control signals, making the approach effective for solving complex tasks. Since the introduction of the deep Q-network (DQN) in 2015, DRL has achieved significant successes across domains including games, robotics, and autonomous systems, and is increasingly applied in areas such as healthcare, finance, and autonomous vehicles.\n",
      "\n",
      "\n",
      "\n",
      "Page: Reinforcement learning from human feedback\n",
      "Summary: In machine learning, reinforcement learning from human feedback (RLHF) is a technique to align an intelligent agent with human preferences. It involves training a reward model to represent preferences, which can then be used to train other models through reinforcement learning.\n",
      "In classical reinforcement learning, an intelligent agent's goal is to learn a function that guides its behavior, called a policy. This function is iteratively updated to maximize rewards based on the agent's task performance. However, explicitly defining a reward function that accurately approximates human preferences is challenging. Therefore, RLHF seeks to train a \"reward model\" directly from human feedback. The reward model is first trained in a supervised manner to predict if a response to a given prompt is good (high reward) or bad (low reward) based on ranking data collected from human annotators. This model then serves as a reward function to improve an agent's policy through an optimization algorithm like proximal policy optimization.\n",
      "\n",
      "RLHF has applications in various domains in machine learning, including natural language processing tasks such as text summarization and conversational agents, computer vision tasks like text-to-image models, and the development of video game bots. While RLHF is an effective method of training models to act better in accordance with human preferences, it also faces challenges due to the way the human preference data is collected. Though RLHF does not require massive amounts of data to improve performance, sourcing high-quality preference data is still an expensive process. Furthermore, if the data is not carefully collected from a representative sample, the resulting model may exhibit unwanted biases.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Update from node agent:\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: research_agent\n",
      "\n",
      "Reinforcement learning (RL) is a subfield of machine learning and optimal control that focuses on how an intelligent agent should take actions in a dynamic environment to maximize a reward signal. It is one of the three main paradigms of machine learning, alongside supervised and unsupervised learning.\n",
      "\n",
      "In reinforcement learning, an agent learns to make decisions by exploring different actions and receiving feedback in the form of rewards or penalties. Unlike supervised learning, RL does not rely on labeled input-output pairs, nor does it require explicit corrections for sub-optimal actions. Instead, it emphasizes a balance between exploration (trying new actions) and exploitation (using known information) to maximize cumulative rewards.\n",
      "\n",
      "The framework for most reinforcement learning problems is described through a Markov decision process (MDP), which helps in managing the complexities involved in decision-making processes. A key distinction between traditional dynamic programming and RL algorithms is that RL does not assume knowledge of a precise mathematical model of the MDP, making it suitable for larger and more complex decision problems.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = {\"recursion_limit\": 10}\n",
    "for chunk in research_agent.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What is reinforcement learning?\",\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    config=config,\n",
    "):\n",
    "    pretty_print_messages(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a473b3",
   "metadata": {},
   "source": [
    "**Visual Agent AI**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b44b901c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "from torch import imag\n",
    "\n",
    "def encode_image(image_path_or_url: str, get_nine_type: bool = False):\n",
    "    if image_path_or_url.startswith(\"http\"):\n",
    "        try:\n",
    "            response = requests.get(image_path_or_url)\n",
    "            response.raise_for_status()\n",
    "            image = response.content\n",
    "            nine_type = response.headers.get(\"Content-Type\", None)\n",
    "            base64_encode = base64.b64encode(image).decode(\"utf-8\")\n",
    "            if get_nine_type:\n",
    "                return base64_encode, nine_type\n",
    "            return base64_encode\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request_error: {e}\")\n",
    "            if get_nine_type:\n",
    "                return None, None\n",
    "            return None\n",
    "    else:\n",
    "        try:\n",
    "            nine_type = magic.Magic(mime=True).from_file(image_path_or_url)\n",
    "            if nine_type.startswith(\"image/\"):\n",
    "                with open(image_path_or_url, \"rb\") as f:\n",
    "                    image = f.read()\n",
    "                    base64_encode = base64.b64encode(image).decode(\"utf-8\")\n",
    "                    if get_nine_type:\n",
    "                        return base64_encode, None\n",
    "                return base64_encode\n",
    "        except Exception as e:\n",
    "            print(f\"File_error: {e}\")\n",
    "            if get_nine_type:\n",
    "                return None, None\n",
    "            return None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c3764e",
   "metadata": {},
   "source": [
    "**URL Extractor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "49efd0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-nano\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "dff84a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageInput(image_path_or_url='https://example.com/image.png')"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class ImageInput(BaseModel):\n",
    "    image_path_or_url: str = Field(description=\"Path to image or URL\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=ImageInput)\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Extract the image path or URL from the following input:\\n\\n{input}\\n\\n{format_instructions}\"\n",
    ").partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "extractor_chain = prompt | llm | parser\n",
    "output = extractor_chain.invoke({\"input\": 'image_path: \"https://example.com/image.png\"'})\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e723cf",
   "metadata": {},
   "source": [
    "**Describer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c307f9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "import requests\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "class ImageDescription(BaseModel):\n",
    "    image_description: str = Field(description=\"Detailed description of the image\")\n",
    "\n",
    "def image_describer_chat_template_func(input: dict):\n",
    "    image_path_or_url = input[\"image_path_or_url\"]\n",
    "    image_b64, image_mime_type = encode_image(image_path_or_url, get_nine_type = True)\n",
    "\n",
    "    image_describer_chat_template = ChatPromptTemplate.from_messages([\n",
    "        SystemMessage(\n",
    "            content=\"\"\"You are an expert image describer. When presented with an image, provide a detailed, accurate, and objective description of its visible content. Focus on aspects such as:\n",
    "            - Objects present, their positions, and relationships\n",
    "            - Colors, lighting, composition, and textures\n",
    "            - Actions or dynamics, if any (e.g., people walking, water flowing)\n",
    "            - Contextual or inferred information (e.g., likely setting, era, or activity)\n",
    "\n",
    "            Avoid adding information that is not visible or cannot be reasonably inferred from the image. Do not speculate or inject personal opinion unless explicitly requested. If text appears in the image, transcribe it accurately.\"\"\"),\n",
    "        HumanMessage(content=[\n",
    "            {\"type\": \"text\", \"text\": \"Describe the following image for me:\"},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:{image_mime_type};base64,{image_b64}\", \"detail\": \"low\"}\n",
    "            }\n",
    "        ])\n",
    "    ])\n",
    "\n",
    "    return image_describer_chat_template.invoke({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "0fb4df0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageDescription(image_description=\"A black and white silhouette of a cartoon cat with pointy ears and a curved tail, positioned in front of a solid black circle background. The cat's face is blank, with no facial features, and the overall style is minimalist and icon-like.\")"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_describer_agent = image_describer_chat_template_func | llm.with_structured_output(ImageDescription)\n",
    "image_describer_agent.invoke({\"image_path_or_url\": \"https://github.githubassets.com/assets/GitHub-Mark-ea2971cee799.png\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "5c8c6ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import BaseTool\n",
    "from typing import Optional\n",
    "from langchain_core.tools.base import ArgsSchema\n",
    "from langchain_core.callbacks import AsyncCallbackManagerForToolRun, CallbackManagerForToolRun\n",
    "\n",
    "class ImageDescriberInput(BaseModel):\n",
    "    text: str = Field(description = \"Path or URL to the image in the format PNG or JPG/JPEG\")\n",
    "\n",
    "class ImageDescriberTool(BaseTool):\n",
    "    name: str = \"Image_describer\"\n",
    "    description: str = \"Use this tool to get a description of the image in a detailed way\"\n",
    "    args_schema: Optional[ArgsSchema]  = ImageDescriberInput\n",
    "    return_direct: bool = True\n",
    "    \n",
    "    def _run(self, text: str, run_manager: Optional[CallbackManagerForToolRun] = None) -> str:\n",
    "        try:\n",
    "            parsed: ImageInput = extractor_chain.invoke({\"input\": text})\n",
    "        except Exception as e:\n",
    "            return f\"Failed to extract image URL: {str(e)}\"\n",
    "\n",
    "        image_path_or_url = parsed.image_path_or_url\n",
    "        if not image_path_or_url:\n",
    "            return \"No Image or URL Found in the input\"\n",
    "        output = image_describer_agent.invoke({\"image_path_or_url\": image_path_or_url})\n",
    "        return output.image_description\n",
    "\n",
    "async def _arun(self, image_path_or_url: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None) -> str:\n",
    "    return self._run(image_path_or_url, run_manager = run_manager)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d7ba6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The image contains a black circle with a white silhouette of a cat-like figure inside it. The silhouette features a rounded head with pointy ears, a small rounded body, and a curved tail pointing to the left. The design is simple and minimalistic, resembling the GitHub logo, which is a stylized depiction of a cat or fox.'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_describer_tool = ImageDescriberTool()\n",
    "image_describer_tool.invoke(\"https://github.githubassets.com/assets/GitHub-Mark-ea2971cee799.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0eaf1e",
   "metadata": {},
   "source": [
    "**Detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "7a324d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ultralytics\n",
      "  Downloading ultralytics-8.3.145-py3-none-any.whl.metadata (37 kB)\n",
      "Requirement already satisfied: numpy>=1.23.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ultralytics) (2.2.3)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ultralytics) (3.10.3)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ultralytics) (4.11.0.86)\n",
      "Requirement already satisfied: pillow>=7.1.2 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ultralytics) (11.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ultralytics) (2.32.3)\n",
      "Collecting scipy>=1.4.1 (from ultralytics)\n",
      "  Downloading scipy-1.15.3-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: torch>=1.8.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ultralytics) (2.6.0)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ultralytics) (0.21.0)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ultralytics) (4.67.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ultralytics) (7.0.0)\n",
      "Collecting py-cpuinfo (from ultralytics)\n",
      "  Using cached py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ultralytics) (2.2.3)\n",
      "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
      "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.23.0->ultralytics) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.8.0->ultralytics) (4.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.8.0->ultralytics) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
      "Downloading ultralytics-8.3.145-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.3/1.0 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 0.5/1.0 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.0/1.0 MB 2.0 MB/s eta 0:00:00\n",
      "Downloading scipy-1.15.3-cp311-cp311-win_amd64.whl (41.2 MB)\n",
      "   ---------------------------------------- 0.0/41.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/41.2 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.8/41.2 MB 2.4 MB/s eta 0:00:17\n",
      "   - -------------------------------------- 1.3/41.2 MB 2.4 MB/s eta 0:00:17\n",
      "   -- ------------------------------------- 2.1/41.2 MB 2.6 MB/s eta 0:00:16\n",
      "   -- ------------------------------------- 2.4/41.2 MB 2.3 MB/s eta 0:00:18\n",
      "   -- ------------------------------------- 2.9/41.2 MB 2.4 MB/s eta 0:00:17\n",
      "   --- ------------------------------------ 3.4/41.2 MB 2.4 MB/s eta 0:00:16\n",
      "   --- ------------------------------------ 3.9/41.2 MB 2.4 MB/s eta 0:00:16\n",
      "   ---- ----------------------------------- 4.5/41.2 MB 2.5 MB/s eta 0:00:15\n",
      "   ---- ----------------------------------- 5.0/41.2 MB 2.5 MB/s eta 0:00:15\n",
      "   ----- ---------------------------------- 5.5/41.2 MB 2.5 MB/s eta 0:00:15\n",
      "   ------ --------------------------------- 6.3/41.2 MB 2.5 MB/s eta 0:00:14\n",
      "   ------ --------------------------------- 6.8/41.2 MB 2.5 MB/s eta 0:00:14\n",
      "   ------- -------------------------------- 7.6/41.2 MB 2.6 MB/s eta 0:00:14\n",
      "   ------- -------------------------------- 8.1/41.2 MB 2.6 MB/s eta 0:00:13\n",
      "   -------- ------------------------------- 8.7/41.2 MB 2.6 MB/s eta 0:00:13\n",
      "   --------- ------------------------------ 9.4/41.2 MB 2.6 MB/s eta 0:00:13\n",
      "   --------- ------------------------------ 10.0/41.2 MB 2.6 MB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 10.5/41.2 MB 2.7 MB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 11.3/41.2 MB 2.7 MB/s eta 0:00:12\n",
      "   ----------- ---------------------------- 12.1/41.2 MB 2.7 MB/s eta 0:00:11\n",
      "   ------------ --------------------------- 12.8/41.2 MB 2.8 MB/s eta 0:00:11\n",
      "   ------------- -------------------------- 13.6/41.2 MB 2.8 MB/s eta 0:00:10\n",
      "   ------------- -------------------------- 14.2/41.2 MB 2.8 MB/s eta 0:00:10\n",
      "   -------------- ------------------------- 14.7/41.2 MB 2.8 MB/s eta 0:00:10\n",
      "   -------------- ------------------------- 15.2/41.2 MB 2.8 MB/s eta 0:00:10\n",
      "   --------------- ------------------------ 16.0/41.2 MB 2.8 MB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 16.8/41.2 MB 2.8 MB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 17.6/41.2 MB 2.9 MB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 18.4/41.2 MB 2.9 MB/s eta 0:00:08\n",
      "   ------------------ --------------------- 19.1/41.2 MB 2.9 MB/s eta 0:00:08\n",
      "   ------------------- -------------------- 19.9/41.2 MB 3.0 MB/s eta 0:00:08\n",
      "   -------------------- ------------------- 20.7/41.2 MB 3.0 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 21.5/41.2 MB 3.0 MB/s eta 0:00:07\n",
      "   --------------------- ------------------ 22.3/41.2 MB 3.0 MB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 23.3/41.2 MB 3.1 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 24.1/41.2 MB 3.1 MB/s eta 0:00:06\n",
      "   ------------------------ --------------- 24.9/41.2 MB 3.1 MB/s eta 0:00:06\n",
      "   ------------------------ --------------- 25.7/41.2 MB 3.1 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 26.7/41.2 MB 3.2 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 27.5/41.2 MB 3.2 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 28.6/41.2 MB 3.2 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 29.4/41.2 MB 3.2 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 30.1/41.2 MB 3.2 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 30.9/41.2 MB 3.3 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 31.7/41.2 MB 3.3 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 32.5/41.2 MB 3.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 33.3/41.2 MB 3.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 33.8/41.2 MB 3.3 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 34.6/41.2 MB 3.3 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 35.4/41.2 MB 3.3 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 36.2/41.2 MB 3.3 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 37.0/41.2 MB 3.3 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 37.7/41.2 MB 3.3 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 38.5/41.2 MB 3.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 39.6/41.2 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  40.4/41.2 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  41.2/41.2 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 41.2/41.2 MB 3.3 MB/s eta 0:00:00\n",
      "Downloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
      "Using cached py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Installing collected packages: py-cpuinfo, scipy, ultralytics-thop, ultralytics\n",
      "Successfully installed py-cpuinfo-9.0.0 scipy-1.15.3 ultralytics-8.3.145 ultralytics-thop-2.0.14\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\ASUS\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "44d6d6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file  \n",
      "View Ultralytics Settings with 'yolo settings' or at 'C:\\Users\\ASUS\\AppData\\Roaming\\Ultralytics\\settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x.pt to 'yolo11x.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109M/109M [00:34<00:00, 3.37MB/s] \n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "yolo_model = YOLO(\"yolo11x.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "66671137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://s3.amazonaws.com/cdn-origin-etr.akc.org/wp-content/uploads/2018/04/24144817/American-Staffordshire-Terrier-lying-outdoors-next-to-a-kitten-that-is-playing-with-the-dogs-nose.jpg to 'American-Staffordshire-Terrier-lying-outdoors-next-to-a-kitten-that-is-playing-with-the-dogs-nose.jpg'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61.7k/61.7k [00:00<00:00, 115kB/s]\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\triton\\windows_utils.py:433: UserWarning: Failed to find CUDA.\n",
      "  warnings.warn(\"Failed to find CUDA.\")\n"
     ]
    }
   ],
   "source": [
    "results = yolo_model.predict(\n",
    "    \"https://s3.amazonaws.com/cdn-origin-etr.akc.org/wp-content/uploads/2018/04/24144817/American-Staffordshire-Terrier-lying-outdoors-next-to-a-kitten-that-is-playing-with-the-dogs-nose.jpg\", verbose=False)\n",
    "\n",
    "results[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "60c58c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "class ObjectDetectionAndCountingInput(BaseModel):\n",
    "    text: str = Field(description=\"Path or URL to the image in the format PNG or JPG/JPEG\")\n",
    "\n",
    "@tool(\n",
    "    \"Object Detection and Counting\",\n",
    "    description=\"Use this tool to detect objects in an image and count the number of objects of each type.\",\n",
    "    args_schema=ObjectDetectionAndCountingInput\n",
    ")\n",
    "def detect_and_count_object_tool(\n",
    "    text: Annotated[str, \"Path or URL to the image\"]\n",
    "):\n",
    "    try:\n",
    "        parsed: ImageInput = extractor_chain.invoke({\"input\": text})\n",
    "    except Exception as e:\n",
    "        return f\"Failed to extract image URL: {str(e)}\"\n",
    "    \n",
    "    image_path_or_url = parsed.image_path_or_url\n",
    "    if not image_path_or_url:\n",
    "        return \"No image URL found in the input\"\n",
    "    results = yolo_model(image_path_or_url, verbose=False)\n",
    "\n",
    "    detection = []\n",
    "    counting = {}\n",
    "\n",
    "    for result in results:\n",
    "        boxes = result.boxes\n",
    "        class_names = result.names\n",
    "        for box in boxes:\n",
    "            class_id = int(box.cls[0])\n",
    "            class_name = class_names[class_id]\n",
    "            confidence = float(box.conf[0])\n",
    "            x1 ,y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            detection.append({\n",
    "                \"class_name\": class_name,\n",
    "                \"confidence\": confidence,\n",
    "                \"bbox\": [x1, y1, x2, y2]\n",
    "            })\n",
    "          \n",
    "            counting[class_name] = counting.get(class_name, 0) + 1\n",
    "    \n",
    "    return str({\n",
    "        \"detection\": detection,\n",
    "        \"counting\": counting\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "b17536b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found https://s3.amazonaws.com/cdn-origin-etr.akc.org/wp-content/uploads/2018/04/24144817/American-Staffordshire-Terrier-lying-outdoors-next-to-a-kitten-that-is-playing-with-the-dogs-nose.jpg locally at American-Staffordshire-Terrier-lying-outdoors-next-to-a-kitten-that-is-playing-with-the-dogs-nose.jpg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"{'detection': [{'class_name': 'dog', 'confidence': 0.9411599040031433, 'bbox': [287, 73, 618, 445]}, {'class_name': 'dog', 'confidence': 0.8354384899139404, 'bbox': [159, 120, 369, 418]}], 'counting': {'dog': 2}}\""
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect_and_count_object_tool.invoke(\"https://s3.amazonaws.com/cdn-origin-etr.akc.org/wp-content/uploads/2018/04/24144817/American-Staffordshire-Terrier-lying-outdoors-next-to-a-kitten-that-is-playing-with-the-dogs-nose.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "247e5677",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_agent_visual_assistant = create_react_agent(\n",
    "    model = \"gpt-4o-mini\",\n",
    "    tools = [image_describer_tool, detect_and_count_object_tool],\n",
    "    prompt = (\n",
    "        \"You are a multi_agent visual assistant.\"\n",
    "        \"Assist only with visual task (e.g. image description, object detection and counting).\"\n",
    "        \"Do not assist with non-visual task (e.g. text processing, data analysis, etc.)\"\n",
    "        \"After completing your task respond to the supervisor with the result.\"\n",
    "        \"Respond ONLY with the results of your works, DO NOT include any explanation or additional information.\\n\"\n",
    "    ),\n",
    "    name = \"multi_agent_visual_assistant\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "929dc917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then create the agent\n",
    "multi_agent_visual_assistant = create_react_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[image_describer_tool, detect_and_count_object_tool],\n",
    "    prompt=(\n",
    "        \"You are a multi_agent visual assistant. \"\n",
    "        \"Assist only with visual tasks (e.g., image description, object detection and counting). \"\n",
    "        \"Do not assist with non-visual tasks (e.g., text processing, data analysis, etc.). \"\n",
    "        \"After completing your task, respond to the supervisor with the result. \"\n",
    "        \"Respond ONLY with the results of your work, DO NOT include any explanation or additional information.\"\n",
    "    ),\n",
    "    name=\"multi_agent_visual_assistant\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "ef1086b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update from node agent:\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: multi_agent_visual_assistant\n",
      "Tool Calls:\n",
      "  detect_and_count_objects (call_e4at7QefFN5auWnjE0JIGgA4)\n",
      " Call ID: call_e4at7QefFN5auWnjE0JIGgA4\n",
      "  Args:\n",
      "    text: https://s3.amazonaws.com/cdn-origin-etr.akc.org/wp-content/uploads/2018/04/24144817/American-Staffordshire-Terrier-lying-outdoors-next-to-a-kitten-that-is-playing-with-the-dogs-nose.jpg\n",
      "\n",
      "\n",
      "Found https://s3.amazonaws.com/cdn-origin-etr.akc.org/wp-content/uploads/2018/04/24144817/American-Staffordshire-Terrier-lying-outdoors-next-to-a-kitten-that-is-playing-with-the-dogs-nose.jpg locally at American-Staffordshire-Terrier-lying-outdoors-next-to-a-kitten-that-is-playing-with-the-dogs-nose.jpg\n",
      "Update from node tools:\n",
      "\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: detect_and_count_objects\n",
      "\n",
      "{'detection': [{'class_name': 'dog', 'confidence': 0.9411599040031433, 'bbox': [287, 73, 618, 445]}, {'class_name': 'dog', 'confidence': 0.8354384899139404, 'bbox': [159, 120, 369, 418]}], 'counting': {'dog': 2}}\n",
      "\n",
      "\n",
      "Update from node agent:\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: multi_agent_visual_assistant\n",
      "\n",
      "There are 2 dogs in the image.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for chunk in multi_agent_visual_assistant.stream(\n",
    "    {\n",
    "        \"messages\" : [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"How many Dog in this image? : https://s3.amazonaws.com/cdn-origin-etr.akc.org/wp-content/uploads/2018/04/24144817/American-Staffordshire-Terrier-lying-outdoors-next-to-a-kitten-that-is-playing-with-the-dogs-nose.jpg\",\n",
    "        },]\n",
    "    },\n",
    "):\n",
    "    pretty_print_messages(chunk)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9601e770",
   "metadata": {},
   "source": [
    "**Supervisor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "0db72936",
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisor = create_supervisor(\n",
    "    model = init_chat_model(\"gpt-4o-mini\"),\n",
    "    agents = [research_agent, multi_agent_visual_assistant],\n",
    "    prompt = (\n",
    "        \"You are a supervisor managing two agents:\\n\"\n",
    "        \"- a research agent. Assign research-related tasks to this agent\\n\"\n",
    "        \"- a math agent. Assign math-related tasks to this agent\\n\"\n",
    "        \"Assign work to one agent at a time, do not call agents in parallel.\\n\"\n",
    "        \"Do not do any work yourself.\\n\"\n",
    "        \"Do not call multiple agents in parallel.\\n\"\n",
    "        \"Do not perform any work yourself.\\n\"\n",
    "        \"Be concise and do not include any additional information, ONLY provide what is necessary for them to complete the job.\\n\"\n",
    "    ),\n",
    "    add_handoff_back_messages=True,\n",
    "    output_mode=\"full_history\",\n",
    ").compile()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "e8a972b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update from node supervisor:\n",
      "\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: transfer_to_research_agent\n",
      "\n",
      "Successfully transferred to research_agent\n",
      "\n",
      "\n",
      "Update from node research_agent:\n",
      "\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: transfer_back_to_supervisor\n",
      "\n",
      "Successfully transferred back to supervisor\n",
      "\n",
      "\n",
      "Update from node supervisor:\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: supervisor\n",
      "\n",
      "The United States has a population that constitutes approximately 4.23% of the total world population.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk in supervisor.stream(\n",
    "    {\n",
    "        \"messages\":[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What is percentile of USA in this picture? : https://th.bing.com/th/id/OIP.X9MJLjA_o0DCx-Aq7-6DpAHaId?cb=iwc2&rs=1&pid=ImgDetMain\"\n",
    "            },\n",
    "        ]\n",
    "    },\n",
    "):\n",
    "    pretty_print_messages(chunk, last_message=True)\n",
    "final_message_history = chunk[\"supervisor\"][\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "15d49d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update from node supervisor:\n",
      "\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: transfer_to_research_agent\n",
      "\n",
      "Successfully transferred to research_agent\n",
      "\n",
      "\n",
      "Update from node research_agent:\n",
      "\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: transfer_back_to_supervisor\n",
      "\n",
      "Successfully transferred back to supervisor\n",
      "\n",
      "\n",
      "Update from node supervisor:\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: supervisor\n",
      "\n",
      "I have gathered detailed information about ropes, including their significance, advancements in materials science, and relevant research papers. If you need further assistance, please let me know!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk in supervisor.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What is the concept visualized in the image? Image: https://huggingface.co/datasets/tmnam20/Storage/resolve/main/rope.png Provide me detailed information about the concept. If possible, give me some research papers about it.\",\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "):\n",
    "    pretty_print_messages(chunk, last_message=True)\n",
    "\n",
    "final_message_history = chunk[\"supervisor\"][\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "4b7179d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update from node supervisor:\n",
      "\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: transfer_to_research_agent\n",
      "\n",
      "Successfully transferred to research_agent\n",
      "\n",
      "\n",
      "Update from node research_agent:\n",
      "\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: transfer_back_to_supervisor\n",
      "\n",
      "Successfully transferred back to supervisor\n",
      "\n",
      "\n",
      "Update from node supervisor:\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: supervisor\n",
      "\n",
      "Now I will assign a task to the math agent. \n",
      "\n",
      "Calculating the area of a rectangle with a length of 10 units and a width of 5 units. Please perform this calculation.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk in supervisor.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"How many dogs are there in the image? Image: https://github.githubassets.com/assets/GitHub-Mark-ea2971cee799.png\",\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "):\n",
    "    pretty_print_messages(chunk, last_message=True)\n",
    "\n",
    "final_message_history = chunk[\"supervisor\"][\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "7cdaa97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update from node supervisor:\n",
      "\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: transfer_to_multi_agent_visual_assistant\n",
      "\n",
      "Successfully transferred to multi_agent_visual_assistant\n",
      "\n",
      "\n",
      "Found https://s3.amazonaws.com/cdn-origin-etr.akc.org/wp-content/uploads/2018/04/24144817/American-Staffordshire-Terrier-lying-outdoors-next-to-a-kitten-that-is-playing-with-the-dogs-nose.jpg locally at American-Staffordshire-Terrier-lying-outdoors-next-to-a-kitten-that-is-playing-with-the-dogs-nose.jpg\n",
      "Update from node multi_agent_visual_assistant:\n",
      "\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: transfer_back_to_supervisor\n",
      "\n",
      "Successfully transferred back to supervisor\n",
      "\n",
      "\n",
      "Update from node supervisor:\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: supervisor\n",
      "\n",
      "The image contains 2 dogs.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk in supervisor.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"How many dogs are there in the image? Image: https://s3.amazonaws.com/cdn-origin-etr.akc.org/wp-content/uploads/2018/04/24144817/American-Staffordshire-Terrier-lying-outdoors-next-to-a-kitten-that-is-playing-with-the-dogs-nose.jpg\",\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "):\n",
    "    pretty_print_messages(chunk, last_message=True)\n",
    "\n",
    "final_message_history = chunk[\"supervisor\"][\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bffd6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
